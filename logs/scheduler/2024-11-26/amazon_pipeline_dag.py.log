[2024-11-26T08:31:03.581+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:31:03.583+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:31:03.592+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:31:03.585+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:31:03.842+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:31:03.832+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:31:03.847+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:31:03.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.308 seconds
[2024-11-26T08:31:34.209+0000] {processor.py:161} INFO - Started process (PID=448) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:31:34.210+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:31:34.212+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:31:34.212+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:31:34.314+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:31:34.309+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:31:34.316+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:31:34.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.128 seconds
[2024-11-26T08:32:04.932+0000] {processor.py:161} INFO - Started process (PID=715) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:32:04.934+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:32:04.936+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:32:04.936+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:32:05.048+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:32:05.042+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:32:05.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:32:05.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.144 seconds
[2024-11-26T08:32:35.186+0000] {processor.py:161} INFO - Started process (PID=976) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:32:35.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:32:35.188+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:32:35.188+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:32:35.291+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:32:35.286+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:32:35.293+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:32:35.310+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.128 seconds
[2024-11-26T08:33:05.733+0000] {processor.py:161} INFO - Started process (PID=1243) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:33:05.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:33:05.736+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:33:05.735+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:33:05.823+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:33:05.819+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:33:05.825+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:33:05.840+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.111 seconds
[2024-11-26T08:33:35.906+0000] {processor.py:161} INFO - Started process (PID=1515) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:33:35.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:33:35.908+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:33:35.908+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:33:35.994+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:33:35.990+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:33:35.995+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:33:36.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T08:36:11.976+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:36:11.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:36:11.979+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:36:11.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:36:12.140+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:36:12.136+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:36:12.141+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:36:12.153+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.182 seconds
[2024-11-26T08:36:43.535+0000] {processor.py:161} INFO - Started process (PID=443) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:36:43.536+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:36:43.538+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:36:43.537+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:36:43.742+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:36:43.735+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:36:43.744+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:36:43.765+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.235 seconds
[2024-11-26T08:37:15.492+0000] {processor.py:161} INFO - Started process (PID=710) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:37:15.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:37:15.495+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:37:15.495+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:37:15.594+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:37:15.587+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:37:15.596+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:37:15.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.137 seconds
[2024-11-26T08:37:45.696+0000] {processor.py:161} INFO - Started process (PID=977) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:37:45.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:37:45.699+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:37:45.698+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:37:45.786+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:37:45.781+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:37:45.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:37:45.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.109 seconds
[2024-11-26T08:38:16.416+0000] {processor.py:161} INFO - Started process (PID=1244) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:38:16.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:38:16.419+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:38:16.418+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:38:16.501+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:38:16.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:38:16.503+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:38:16.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.104 seconds
[2024-11-26T08:38:47.125+0000] {processor.py:161} INFO - Started process (PID=1511) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:38:47.126+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:38:47.128+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:38:47.128+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:38:47.213+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:38:47.209+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:38:47.214+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:38:47.226+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.106 seconds
[2024-11-26T08:39:17.351+0000] {processor.py:161} INFO - Started process (PID=1778) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:39:17.352+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:39:17.353+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:39:17.353+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:39:17.461+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:39:17.456+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:39:17.463+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:39:17.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.134 seconds
[2024-11-26T08:39:48.188+0000] {processor.py:161} INFO - Started process (PID=2045) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:39:48.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:39:48.191+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:39:48.191+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:39:48.324+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:39:48.312+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:39:48.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:39:48.339+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.157 seconds
[2024-11-26T08:40:18.433+0000] {processor.py:161} INFO - Started process (PID=2318) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:40:18.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:40:18.435+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:40:18.435+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:40:18.522+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:40:18.518+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
  File "/opt/airflow/dags/tasks/spark_milvus_task.py", line 1, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
[2024-11-26T08:40:18.523+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:40:18.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T08:48:13.669+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:48:13.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:48:13.672+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:13.672+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:48:16.567+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:48:19.573+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:48:19.761+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:19.760+0000] {override.py:1829} INFO - Created Permission View: can edit on DAG:amazon_pipeline
[2024-11-26T08:48:19.768+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:19.768+0000] {override.py:1829} INFO - Created Permission View: can delete on DAG:amazon_pipeline
[2024-11-26T08:48:19.773+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:19.773+0000] {override.py:1829} INFO - Created Permission View: can read on DAG:amazon_pipeline
[2024-11-26T08:48:19.774+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:19.773+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:48:19.782+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:19.781+0000] {dag.py:3118} INFO - Creating ORM DAG for amazon_pipeline
[2024-11-26T08:48:19.791+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:19.791+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-25 00:00:00+00:00, run_after=2024-11-26 00:00:00+00:00
[2024-11-26T08:48:19.805+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 6.143 seconds
[2024-11-26T08:48:50.005+0000] {processor.py:161} INFO - Started process (PID=722) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:48:50.006+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:48:50.008+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:50.008+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:48:51.943+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:48:53.162+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:48:53.183+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:53.183+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:48:53.199+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:48:53.198+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:48:53.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.214 seconds
[2024-11-26T08:49:24.075+0000] {processor.py:161} INFO - Started process (PID=1030) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:49:24.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:49:24.078+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:49:24.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:49:26.060+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:49:27.321+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:49:27.338+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:49:27.337+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:49:27.356+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:49:27.355+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:49:27.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.301 seconds
[2024-11-26T08:49:58.045+0000] {processor.py:161} INFO - Started process (PID=1309) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:49:58.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:49:58.047+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:49:58.047+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:49:59.568+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:50:00.656+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:50:00.667+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:50:00.667+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:50:00.681+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:50:00.681+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:50:00.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.651 seconds
[2024-11-26T08:50:30.840+0000] {processor.py:161} INFO - Started process (PID=1588) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:50:30.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:50:30.842+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:50:30.842+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:50:32.299+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:50:33.392+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:50:33.403+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:50:33.403+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:50:33.417+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:50:33.417+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:50:33.547+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.712 seconds
[2024-11-26T08:51:04.040+0000] {processor.py:161} INFO - Started process (PID=1867) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:51:04.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:51:04.043+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:51:04.043+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:51:05.642+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:51:06.680+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:51:06.692+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:51:06.692+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:51:06.706+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:51:06.706+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:51:06.717+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.682 seconds
[2024-11-26T08:51:36.989+0000] {processor.py:161} INFO - Started process (PID=2146) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:51:36.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:51:36.991+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:51:36.990+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:51:38.592+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:51:39.803+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:51:39.815+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:51:39.815+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:51:39.829+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:51:39.829+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:51:39.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.857 seconds
[2024-11-26T08:52:10.063+0000] {processor.py:161} INFO - Started process (PID=2425) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:52:10.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:52:10.065+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:52:10.065+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:52:11.747+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:52:13.061+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:52:13.079+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:52:13.078+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:52:13.098+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:52:13.098+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:52:13.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.057 seconds
[2024-11-26T08:52:43.295+0000] {processor.py:161} INFO - Started process (PID=2718) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:52:43.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:52:43.298+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:52:43.297+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:52:44.772+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:52:45.826+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:52:45.838+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:52:45.837+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:52:45.852+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:52:45.852+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:52:45.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.693 seconds
[2024-11-26T08:53:16.135+0000] {processor.py:161} INFO - Started process (PID=2997) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:53:16.135+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:53:16.137+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:53:16.137+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:53:17.531+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:53:18.544+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:53:18.556+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:53:18.555+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:53:18.570+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:53:18.570+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:53:18.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.453 seconds
[2024-11-26T08:53:49.152+0000] {processor.py:161} INFO - Started process (PID=3276) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:53:49.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:53:49.154+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:53:49.154+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:53:50.538+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:53:51.668+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:53:51.679+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:53:51.679+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:53:51.692+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:53:51.691+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:53:51.703+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.554 seconds
[2024-11-26T08:54:22.349+0000] {processor.py:161} INFO - Started process (PID=3583) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:54:22.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:54:22.352+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:54:22.352+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:54:23.931+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:54:25.156+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:54:25.320+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:54:25.319+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:54:25.342+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:54:25.342+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:54:25.357+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.013 seconds
[2024-11-26T08:54:56.046+0000] {processor.py:161} INFO - Started process (PID=3862) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:54:56.047+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:54:56.049+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:54:56.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:54:57.532+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:54:58.893+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:54:58.907+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:54:58.907+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:54:59.053+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:54:59.053+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:54:59.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.024 seconds
[2024-11-26T08:55:29.257+0000] {processor.py:161} INFO - Started process (PID=4141) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:55:29.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:55:29.259+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:55:29.259+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:55:29.280+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:55:29.278+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 4, in <module>
    from tasks.scraper_task import scrape_amazon
  File "/opt/airflow/dags/tasks/scraper_task.py", line 14
    print(" Status code :  " +  response.status_code
         ^
SyntaxError: '(' was never closed
[2024-11-26T08:55:29.281+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:55:29.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.045 seconds
[2024-11-26T08:55:59.405+0000] {processor.py:161} INFO - Started process (PID=4409) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:55:59.406+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:55:59.408+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:55:59.407+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:55:59.425+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:55:59.424+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 4, in <module>
    from tasks.scraper_task import scrape_amazon
  File "/opt/airflow/dags/tasks/scraper_task.py", line 14
    print(" Status code :  " +  response.status_code
         ^
SyntaxError: '(' was never closed
[2024-11-26T08:55:59.426+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:55:59.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.039 seconds
[2024-11-26T08:56:29.689+0000] {processor.py:161} INFO - Started process (PID=4676) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:56:29.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:56:29.691+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:56:29.691+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:56:31.342+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:56:32.855+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:56:32.929+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:56:32.928+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:56:32.941+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:56:32.941+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:56:32.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.270 seconds
[2024-11-26T08:57:03.916+0000] {processor.py:161} INFO - Started process (PID=4955) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:57:03.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:57:03.917+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:57:03.917+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:57:07.877+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:57:09.090+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:57:09.103+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:57:09.103+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:57:09.119+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:57:09.119+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:57:09.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 5.220 seconds
[2024-11-26T08:57:39.205+0000] {processor.py:161} INFO - Started process (PID=5235) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:57:39.206+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:57:39.207+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:57:39.206+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:57:43.940+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:57:45.008+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:57:45.019+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:57:45.019+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:57:45.152+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:57:45.151+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:57:45.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 5.962 seconds
[2024-11-26T08:58:15.267+0000] {processor.py:161} INFO - Started process (PID=5549) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:58:15.268+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:58:15.269+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:58:15.269+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:58:19.087+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:58:22.698+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:58:22.712+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:58:22.712+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:58:22.727+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:58:22.727+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:58:22.869+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 5.079 seconds
[2024-11-26T08:58:55.330+0000] {processor.py:161} INFO - Started process (PID=5828) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:58:55.331+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:58:55.332+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:58:55.332+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:59:00.010+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:59:01.197+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:59:01.211+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:59:01.210+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:59:01.227+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:59:01.227+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:59:01.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 5.916 seconds
[2024-11-26T08:59:31.471+0000] {processor.py:161} INFO - Started process (PID=6471) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:59:31.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T08:59:31.473+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:59:31.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:59:36.265+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T08:59:37.329+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T08:59:37.342+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:59:37.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T08:59:37.538+0000] {logging_mixin.py:188} INFO - [2024-11-26T08:59:37.538+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T08:59:37.571+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 6.104 seconds
[2024-11-26T09:00:08.283+0000] {processor.py:161} INFO - Started process (PID=6750) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:00:08.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:00:08.284+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:00:08.284+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:00:13.446+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:00:14.663+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:00:14.676+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:00:14.675+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:00:14.692+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:00:14.692+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:00:14.705+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 6.427 seconds
[2024-11-26T09:00:44.778+0000] {processor.py:161} INFO - Started process (PID=7029) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:00:44.779+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:00:44.781+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:00:44.780+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:00:50.152+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:00:51.426+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:00:51.438+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:00:51.438+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:00:51.453+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:00:51.453+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:00:51.468+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 6.695 seconds
[2024-11-26T09:01:21.664+0000] {processor.py:161} INFO - Started process (PID=7327) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:01:21.664+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:01:21.665+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:01:21.665+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:01:23.161+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:01:24.483+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:01:24.498+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:01:24.498+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:01:24.513+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:01:24.512+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:01:24.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.867 seconds
[2024-11-26T09:03:45.908+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:03:45.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:03:45.911+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:03:45.911+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:03:50.653+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:03:53.407+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:03:53.597+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:03:53.597+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:03:53.614+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:03:53.614+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:03:53.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 5.022 seconds
[2024-11-26T09:04:25.752+0000] {processor.py:161} INFO - Started process (PID=737) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:04:25.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:04:25.756+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:04:25.755+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:04:27.912+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:04:29.269+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:04:29.281+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:04:29.281+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:04:29.295+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:04:29.295+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:04:29.308+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.562 seconds
[2024-11-26T09:04:59.574+0000] {processor.py:161} INFO - Started process (PID=1126) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:04:59.574+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:04:59.576+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:04:59.576+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:05:01.214+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:05:02.510+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:05:02.524+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:05:02.524+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:05:02.543+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:05:02.543+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:05:02.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.993 seconds
[2024-11-26T09:05:33.043+0000] {processor.py:161} INFO - Started process (PID=1405) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:05:33.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:05:33.045+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:05:33.045+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:05:34.616+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:05:35.696+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:05:35.709+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:05:35.709+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:05:35.723+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:05:35.723+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:05:35.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.696 seconds
[2024-11-26T09:06:06.088+0000] {processor.py:161} INFO - Started process (PID=1684) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:06:06.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:06:06.089+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:06:06.089+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:06:07.504+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:06:08.581+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:06:08.594+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:06:08.594+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:06:08.609+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:06:08.609+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:06:08.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.679 seconds
[2024-11-26T09:06:39.562+0000] {processor.py:161} INFO - Started process (PID=1986) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:06:39.563+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:06:39.564+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:06:39.564+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:06:41.212+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:06:42.199+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:06:42.211+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:06:42.210+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:06:42.224+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:06:42.224+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:06:42.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.678 seconds
[2024-11-26T09:07:12.426+0000] {processor.py:161} INFO - Started process (PID=2265) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:07:12.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:07:12.428+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:07:12.428+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:07:13.824+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:07:14.847+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:07:14.861+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:07:14.860+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:07:14.875+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:07:14.875+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:07:14.886+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.464 seconds
[2024-11-26T09:07:45.289+0000] {processor.py:161} INFO - Started process (PID=2544) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:07:45.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:07:45.291+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:07:45.291+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:07:46.724+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:07:47.771+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:07:47.788+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:07:47.788+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:07:47.803+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:07:47.803+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:07:47.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.531 seconds
[2024-11-26T09:08:17.881+0000] {processor.py:161} INFO - Started process (PID=2823) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:08:17.882+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:08:17.883+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:08:17.883+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:08:19.296+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:08:20.421+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:08:20.436+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:08:20.435+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:08:20.573+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:08:20.573+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:08:20.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.708 seconds
[2024-11-26T09:08:51.302+0000] {processor.py:161} INFO - Started process (PID=3117) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:08:51.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:08:51.305+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:08:51.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:08:52.741+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:08:53.756+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:08:53.768+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:08:53.768+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:08:53.782+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:08:53.782+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:08:53.909+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.612 seconds
[2024-11-26T09:09:24.688+0000] {processor.py:161} INFO - Started process (PID=3435) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:09:24.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:09:24.693+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:09:24.692+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:09:26.514+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:09:27.889+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:09:27.927+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:09:27.926+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:09:28.013+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:09:28.012+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:09:28.360+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.679 seconds
[2024-11-26T09:09:59.034+0000] {processor.py:161} INFO - Started process (PID=3800) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:09:59.034+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:09:59.036+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:09:59.036+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:10:00.646+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:10:01.854+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:10:01.866+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:10:01.865+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:10:01.880+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:10:01.879+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:10:01.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.862 seconds
[2024-11-26T09:10:32.590+0000] {processor.py:161} INFO - Started process (PID=4089) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:10:32.591+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:10:32.592+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:10:32.592+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:10:34.016+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:10:35.208+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:10:35.222+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:10:35.221+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:10:35.237+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:10:35.236+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:10:35.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.664 seconds
[2024-11-26T09:11:06.081+0000] {processor.py:161} INFO - Started process (PID=4368) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:11:06.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:11:06.083+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:11:06.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:11:07.503+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:11:08.703+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:11:08.716+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:11:08.716+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:11:08.731+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:11:08.731+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:11:08.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.665 seconds
[2024-11-26T09:11:39.169+0000] {processor.py:161} INFO - Started process (PID=4660) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:11:39.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:11:39.171+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:11:39.171+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:11:40.751+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:11:41.866+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:11:41.880+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:11:41.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:11:42.035+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:11:42.034+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:11:42.047+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.883 seconds
[2024-11-26T09:12:12.674+0000] {processor.py:161} INFO - Started process (PID=4949) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:12:12.675+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:12:12.676+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:12:12.676+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:12:14.260+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:12:15.317+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:12:15.329+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:12:15.329+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:12:15.343+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:12:15.343+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:12:15.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.820 seconds
[2024-11-26T09:12:46.057+0000] {processor.py:161} INFO - Started process (PID=5228) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:12:46.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:12:46.059+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:12:46.059+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:12:47.466+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:12:48.692+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:12:48.705+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:12:48.705+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:12:48.721+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:12:48.721+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:12:48.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.680 seconds
[2024-11-26T09:13:18.848+0000] {processor.py:161} INFO - Started process (PID=5507) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:13:18.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:13:18.851+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:13:18.850+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:13:20.319+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:13:21.696+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:13:21.712+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:13:21.711+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:13:21.730+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:13:21.730+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:13:21.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.903 seconds
[2024-11-26T09:13:51.940+0000] {processor.py:161} INFO - Started process (PID=5786) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:13:51.941+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:13:51.942+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:13:51.942+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:13:53.350+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:13:54.481+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:13:54.493+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:13:54.492+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:13:54.506+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:13:54.506+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:13:54.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.582 seconds
[2024-11-26T09:14:24.760+0000] {processor.py:161} INFO - Started process (PID=6087) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:14:24.761+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:14:24.763+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:14:24.762+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:14:26.342+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:14:27.448+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:14:27.461+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:14:27.460+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:14:27.597+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:14:27.597+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:14:27.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.854 seconds
[2024-11-26T09:14:58.109+0000] {processor.py:161} INFO - Started process (PID=6470) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:14:58.110+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:14:58.112+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:14:58.112+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:14:59.560+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:15:00.758+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:15:00.773+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:15:00.773+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:15:00.790+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:15:00.790+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:15:00.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.704 seconds
[2024-11-26T09:15:31.616+0000] {processor.py:161} INFO - Started process (PID=6749) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:15:31.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:15:31.618+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:15:31.618+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:15:33.302+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:15:34.895+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:15:34.911+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:15:34.911+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:15:34.928+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:15:34.928+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:15:34.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.332 seconds
[2024-11-26T09:16:04.992+0000] {processor.py:161} INFO - Started process (PID=7028) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:16:04.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:16:04.994+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:16:04.994+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:16:06.625+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:16:07.920+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:16:07.933+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:16:07.932+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:16:07.947+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:16:07.947+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:16:07.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.972 seconds
[2024-11-26T09:16:38.381+0000] {processor.py:161} INFO - Started process (PID=7327) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:16:38.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:16:38.383+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:16:38.383+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:16:39.906+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:16:41.223+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:16:41.236+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:16:41.236+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:16:41.253+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:16:41.253+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:16:41.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.893 seconds
[2024-11-26T09:17:11.580+0000] {processor.py:161} INFO - Started process (PID=7628) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:17:11.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:17:11.583+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:17:11.582+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:17:13.010+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:17:14.209+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:17:14.221+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:17:14.220+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:17:14.240+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:17:14.240+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:17:14.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.684 seconds
[2024-11-26T09:17:45.078+0000] {processor.py:161} INFO - Started process (PID=7907) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:17:45.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:17:45.081+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:17:45.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:17:46.690+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:17:47.861+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:17:47.877+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:17:47.877+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:17:47.892+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:17:47.892+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:17:47.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.830 seconds
[2024-11-26T09:18:18.805+0000] {processor.py:161} INFO - Started process (PID=8196) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:18:18.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:18:18.808+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:18:18.807+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:18:20.147+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:18:21.258+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:18:21.270+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:18:21.270+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:18:21.285+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:18:21.285+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:18:21.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.497 seconds
[2024-11-26T09:18:52.093+0000] {processor.py:161} INFO - Started process (PID=8475) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:18:52.094+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:18:52.095+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:18:52.095+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:18:53.470+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:18:54.596+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:18:54.608+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:18:54.607+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:18:54.621+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:18:54.621+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:18:54.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.543 seconds
[2024-11-26T09:19:25.331+0000] {processor.py:161} INFO - Started process (PID=8764) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:19:25.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:19:25.334+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:19:25.333+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:19:26.774+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:19:27.914+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:19:27.931+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:19:27.931+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:19:27.947+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:19:27.947+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:19:27.961+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.635 seconds
[2024-11-26T09:19:58.618+0000] {processor.py:161} INFO - Started process (PID=9080) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:19:58.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:19:58.620+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:19:58.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:20:00.180+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:20:01.429+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:20:01.443+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:20:01.442+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:20:01.458+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:20:01.458+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:20:01.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.856 seconds
[2024-11-26T09:20:32.121+0000] {processor.py:161} INFO - Started process (PID=9359) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:20:32.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:20:32.123+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:20:32.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:20:33.463+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:20:34.590+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:20:34.602+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:20:34.601+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:20:34.616+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:20:34.616+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:20:34.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.510 seconds
[2024-11-26T09:21:05.213+0000] {processor.py:161} INFO - Started process (PID=9638) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:21:05.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:21:05.215+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:21:05.215+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:21:06.643+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:21:07.794+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:21:07.806+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:21:07.806+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:21:07.819+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:21:07.819+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:21:07.830+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.621 seconds
[2024-11-26T09:21:37.889+0000] {processor.py:161} INFO - Started process (PID=9917) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:21:37.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:21:37.891+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:21:37.891+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:21:39.285+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:21:40.431+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:21:40.443+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:21:40.443+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:21:40.458+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:21:40.458+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:21:40.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.584 seconds
[2024-11-26T09:22:10.947+0000] {processor.py:161} INFO - Started process (PID=10196) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:22:10.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:22:10.949+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:22:10.948+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:22:12.290+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:22:13.404+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:22:13.417+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:22:13.417+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:22:13.430+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:22:13.430+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:22:13.442+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.500 seconds
[2024-11-26T09:22:43.807+0000] {processor.py:161} INFO - Started process (PID=10475) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:22:43.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:22:43.809+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:22:43.809+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:22:45.396+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:22:46.656+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:22:46.669+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:22:46.669+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:22:46.684+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:22:46.684+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:22:46.696+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.893 seconds
[2024-11-26T09:23:16.889+0000] {processor.py:161} INFO - Started process (PID=10767) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:23:16.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:23:16.890+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:23:16.890+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:23:18.242+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:23:19.431+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:23:19.442+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:23:19.442+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:23:19.454+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:23:19.454+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:23:19.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.581 seconds
[2024-11-26T09:23:50.110+0000] {processor.py:161} INFO - Started process (PID=11048) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:23:50.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:23:50.112+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:23:50.112+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:23:51.660+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:23:52.933+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:23:52.945+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:23:52.945+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:23:52.961+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:23:52.960+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:23:52.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.869 seconds
[2024-11-26T09:24:23.215+0000] {processor.py:161} INFO - Started process (PID=11337) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:24:23.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:24:23.217+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:24:23.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:24:24.561+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:24:25.770+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:24:25.783+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:24:25.782+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:24:25.797+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:24:25.797+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:24:25.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.600 seconds
[2024-11-26T09:24:56.091+0000] {processor.py:161} INFO - Started process (PID=11616) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:24:56.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:24:56.093+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:24:56.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:24:57.737+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:24:59.039+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:24:59.054+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:24:59.053+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:24:59.069+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:24:59.069+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:24:59.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.995 seconds
[2024-11-26T09:25:30.008+0000] {processor.py:161} INFO - Started process (PID=11895) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:25:30.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:25:30.010+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:25:30.010+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:25:31.700+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:25:32.856+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:25:32.872+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:25:32.872+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:25:32.889+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:25:32.889+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:25:32.904+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.902 seconds
[2024-11-26T09:26:03.654+0000] {processor.py:161} INFO - Started process (PID=12174) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:26:03.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:26:03.656+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:26:03.656+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:26:05.385+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:26:06.832+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:26:06.846+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:26:06.845+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:26:06.861+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:26:06.861+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:26:06.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.227 seconds
[2024-11-26T09:26:36.928+0000] {processor.py:161} INFO - Started process (PID=12453) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:26:36.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:26:36.930+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:26:36.930+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:26:38.586+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:26:39.885+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:26:39.899+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:26:39.898+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:26:39.920+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:26:39.919+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:26:39.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.016 seconds
[2024-11-26T09:27:10.729+0000] {processor.py:161} INFO - Started process (PID=12732) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:27:13.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:27:13.083+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:27:13.082+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:27:14.644+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:27:15.718+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:27:17.471+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:27:17.471+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:27:17.491+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:27:17.491+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:27:17.685+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 6.961 seconds
[2024-11-26T09:27:48.464+0000] {processor.py:161} INFO - Started process (PID=13011) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:27:50.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:27:50.467+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:27:50.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:27:52.001+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:27:53.206+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:27:53.219+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:27:53.219+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:27:53.235+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:27:53.234+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:27:53.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 4.818 seconds
[2024-11-26T09:28:25.956+0000] {processor.py:161} INFO - Started process (PID=13291) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:28:25.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:28:25.966+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:28:25.966+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:28:29.831+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:28:32.580+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:28:32.595+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:28:32.595+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:28:32.614+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:28:32.613+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:28:32.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 6.691 seconds
[2024-11-26T09:29:03.101+0000] {processor.py:161} INFO - Started process (PID=13570) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:29:03.114+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:29:03.118+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:29:03.118+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:29:10.909+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:29:17.979+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:29:18.016+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:29:18.007+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:29:18.082+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:29:18.082+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:29:18.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 11.810 seconds
[2024-11-26T09:29:50.236+0000] {processor.py:161} INFO - Started process (PID=13875) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:29:50.237+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:29:50.240+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:29:50.239+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:29:52.326+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:29:53.576+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:29:53.589+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:29:53.588+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:29:53.603+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:29:53.603+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:29:53.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.388 seconds
[2024-11-26T09:30:24.545+0000] {processor.py:161} INFO - Started process (PID=14279) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:30:24.546+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:30:24.548+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:30:24.548+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:30:26.221+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:30:27.572+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:30:27.589+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:30:27.588+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:30:27.609+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:30:27.609+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:30:27.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.086 seconds
[2024-11-26T09:31:03.942+0000] {processor.py:161} INFO - Started process (PID=14627) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:31:03.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:31:03.960+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:31:03.960+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:31:08.691+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:31:12.492+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:31:12.507+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:31:12.506+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:31:12.524+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:31:12.524+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:31:12.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 8.605 seconds
[2024-11-26T09:31:42.652+0000] {processor.py:161} INFO - Started process (PID=14959) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:31:42.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:31:42.655+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:31:42.654+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:31:44.419+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:31:45.651+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:31:45.669+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:31:45.668+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:31:45.687+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:31:45.686+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:31:45.704+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.059 seconds
[2024-11-26T09:32:16.532+0000] {processor.py:161} INFO - Started process (PID=15253) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:32:16.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:32:16.534+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:32:16.534+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:32:18.118+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:32:19.280+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:32:19.297+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:32:19.296+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:32:19.325+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:32:19.324+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:32:19.386+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.858 seconds
[2024-11-26T09:32:50.363+0000] {processor.py:161} INFO - Started process (PID=15532) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:32:50.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:32:50.367+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:32:50.366+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:32:51.933+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:32:53.079+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:32:53.094+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:32:53.093+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:32:53.111+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:32:53.110+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:32:53.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.770 seconds
[2024-11-26T09:33:23.263+0000] {processor.py:161} INFO - Started process (PID=15811) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:33:23.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:33:23.266+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:33:23.265+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:33:24.805+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:33:25.940+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:33:25.954+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:33:25.953+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:33:25.969+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:33:25.969+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:33:25.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.725 seconds
[2024-11-26T09:33:56.137+0000] {processor.py:161} INFO - Started process (PID=16090) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:33:56.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:33:56.140+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:33:56.140+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:33:57.700+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:33:58.887+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:33:58.906+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:33:58.905+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:33:58.926+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:33:58.925+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:33:58.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.808 seconds
[2024-11-26T09:34:29.738+0000] {processor.py:161} INFO - Started process (PID=16369) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:34:29.739+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:34:29.740+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:34:29.740+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:34:31.173+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:34:32.191+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:34:32.208+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:34:32.208+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:34:32.223+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:34:32.223+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:34:32.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.502 seconds
[2024-11-26T09:35:02.405+0000] {processor.py:161} INFO - Started process (PID=16648) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:35:02.406+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:35:02.407+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:35:02.407+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:35:04.013+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:35:06.024+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:35:06.052+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:35:06.052+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:35:06.110+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:35:06.110+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:35:06.142+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.743 seconds
[2024-11-26T09:35:36.651+0000] {processor.py:161} INFO - Started process (PID=16927) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:35:36.652+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:35:36.653+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:35:36.653+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:35:38.670+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:35:39.912+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:35:39.945+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:35:39.944+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:35:39.959+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:35:39.959+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:35:39.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.341 seconds
[2024-11-26T09:36:10.485+0000] {processor.py:161} INFO - Started process (PID=17206) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:36:10.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:36:10.488+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:36:10.487+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:36:12.217+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:36:13.592+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:36:13.605+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:36:13.605+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:36:13.621+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:36:13.620+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:36:13.633+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.153 seconds
[2024-11-26T09:36:44.477+0000] {processor.py:161} INFO - Started process (PID=17485) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:36:44.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:36:44.480+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:36:44.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:36:45.911+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:36:47.051+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:36:47.062+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:36:47.062+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:36:47.083+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:36:47.083+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:36:47.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.626 seconds
[2024-11-26T09:37:17.724+0000] {processor.py:161} INFO - Started process (PID=17764) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:37:17.725+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:37:17.726+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:37:17.726+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:37:19.198+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:37:20.314+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:37:20.326+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:37:20.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:37:20.340+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:37:20.340+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:37:20.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.634 seconds
[2024-11-26T09:37:50.900+0000] {processor.py:161} INFO - Started process (PID=18043) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:37:50.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:37:50.902+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:37:50.902+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:37:52.333+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:37:53.386+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:37:53.399+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:37:53.398+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:37:53.414+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:37:53.414+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:37:53.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.532 seconds
[2024-11-26T09:38:23.884+0000] {processor.py:161} INFO - Started process (PID=18322) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:38:23.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:38:23.886+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:38:23.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:38:25.457+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:38:26.570+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:38:26.593+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:38:26.593+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:38:26.611+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:38:26.611+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:38:26.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.745 seconds
[2024-11-26T09:38:56.873+0000] {processor.py:161} INFO - Started process (PID=18602) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:38:56.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:38:56.876+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:38:56.876+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:38:58.363+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:38:59.501+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:38:59.515+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:38:59.514+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:38:59.530+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:38:59.530+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:38:59.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.675 seconds
[2024-11-26T09:39:29.675+0000] {processor.py:161} INFO - Started process (PID=19036) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:39:29.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:39:29.677+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:39:29.677+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:39:31.131+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:39:32.197+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:39:32.209+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:39:32.208+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:39:32.224+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:39:32.223+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:39:32.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.565 seconds
[2024-11-26T09:40:02.323+0000] {processor.py:161} INFO - Started process (PID=19350) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:40:02.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:40:02.325+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:40:02.325+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:40:03.755+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:40:04.857+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:40:04.870+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:40:04.869+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:40:04.885+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:40:04.884+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:40:04.897+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.577 seconds
[2024-11-26T09:40:35.360+0000] {processor.py:161} INFO - Started process (PID=19629) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:40:35.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:40:35.362+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:40:35.362+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:40:36.818+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:40:37.903+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:40:37.916+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:40:37.916+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:40:37.931+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:40:37.931+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:40:37.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.590 seconds
[2024-11-26T09:41:08.554+0000] {processor.py:161} INFO - Started process (PID=19909) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:41:08.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:41:08.556+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:41:08.556+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:41:10.463+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:41:11.553+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:41:11.566+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:41:11.565+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:41:11.580+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:41:11.579+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:41:11.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.045 seconds
[2024-11-26T09:41:41.661+0000] {processor.py:161} INFO - Started process (PID=20188) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:41:41.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:41:41.663+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:41:41.663+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:41:43.290+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:41:44.488+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:41:44.504+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:41:44.503+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:41:44.521+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:41:44.521+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:41:44.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.878 seconds
[2024-11-26T09:42:14.770+0000] {processor.py:161} INFO - Started process (PID=20467) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:42:14.771+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:42:14.773+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:42:14.773+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:42:16.646+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:42:17.961+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:42:17.977+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:42:17.977+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:42:17.995+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:42:17.994+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:42:18.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.249 seconds
[2024-11-26T09:42:48.200+0000] {processor.py:161} INFO - Started process (PID=20746) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:42:48.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:42:48.202+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:42:48.202+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:42:49.892+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:42:51.133+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:42:51.147+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:42:51.146+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:42:51.162+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:42:51.162+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:42:51.175+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.980 seconds
[2024-11-26T09:43:21.362+0000] {processor.py:161} INFO - Started process (PID=21025) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:43:21.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:43:21.364+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:43:21.364+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:43:22.946+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:43:24.195+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:43:24.213+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:43:24.212+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:43:24.232+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:43:24.232+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:43:24.248+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.892 seconds
[2024-11-26T09:43:54.406+0000] {processor.py:161} INFO - Started process (PID=21304) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:43:54.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:43:54.408+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:43:54.408+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:43:56.295+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:43:57.559+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:43:57.573+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:43:57.573+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:43:57.590+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:43:57.589+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:43:57.606+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.205 seconds
[2024-11-26T09:44:27.691+0000] {processor.py:161} INFO - Started process (PID=21588) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:44:27.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:44:27.693+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:44:27.693+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:44:29.292+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:44:30.479+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:44:30.493+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:44:30.493+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:44:30.510+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:44:30.509+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:44:30.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.840 seconds
[2024-11-26T09:45:00.589+0000] {processor.py:161} INFO - Started process (PID=21877) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:45:00.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:45:00.591+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:45:00.591+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:45:02.270+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:45:03.515+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:45:03.530+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:45:03.530+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:45:03.548+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:45:03.548+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:45:03.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.980 seconds
[2024-11-26T09:45:33.675+0000] {processor.py:161} INFO - Started process (PID=22156) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:45:33.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:45:33.677+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:45:33.677+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:45:35.110+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:45:36.179+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:45:36.191+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:45:36.191+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:45:36.206+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:45:36.206+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:45:36.219+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.549 seconds
[2024-11-26T09:46:06.286+0000] {processor.py:161} INFO - Started process (PID=22435) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:46:06.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:46:06.289+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:46:06.288+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:46:07.808+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:46:08.928+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:46:08.942+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:46:08.941+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:46:08.958+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:46:08.958+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:46:08.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.688 seconds
[2024-11-26T09:46:39.688+0000] {processor.py:161} INFO - Started process (PID=22714) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:46:39.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:46:39.690+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:46:39.690+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:46:41.391+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:46:42.469+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:46:42.482+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:46:42.482+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:46:42.497+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:46:42.497+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:46:42.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.843 seconds
[2024-11-26T09:47:13.274+0000] {processor.py:161} INFO - Started process (PID=22993) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:47:13.275+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:47:13.277+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:47:13.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:47:14.863+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:47:16.033+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:47:16.048+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:47:16.047+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:47:16.064+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:47:16.064+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:47:16.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.809 seconds
[2024-11-26T09:47:47.004+0000] {processor.py:161} INFO - Started process (PID=23272) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:47:47.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:47:47.007+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:47:47.007+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:47:48.655+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:47:49.929+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:47:49.943+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:47:49.942+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:47:49.960+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:47:49.960+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:47:49.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.975 seconds
[2024-11-26T09:48:20.423+0000] {processor.py:161} INFO - Started process (PID=23551) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:48:20.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:48:20.426+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:48:20.425+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:48:22.080+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:48:23.222+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:48:23.240+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:48:23.239+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:48:23.255+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:48:23.255+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:48:23.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.849 seconds
[2024-11-26T09:48:54.074+0000] {processor.py:161} INFO - Started process (PID=23830) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:48:54.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:48:54.076+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:48:54.076+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:48:55.661+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:48:56.934+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:48:56.976+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:48:56.975+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:48:57.015+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:48:57.015+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:48:57.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.960 seconds
[2024-11-26T09:49:27.291+0000] {processor.py:161} INFO - Started process (PID=24109) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:49:27.292+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:49:27.294+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:49:27.293+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:49:29.978+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:49:31.206+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:49:31.236+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:49:31.236+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:49:31.257+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:49:31.256+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:49:31.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.023 seconds
[2024-11-26T09:50:01.635+0000] {processor.py:161} INFO - Started process (PID=24431) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:50:01.636+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:50:01.637+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:50:01.637+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:50:03.767+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:50:05.620+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:50:05.841+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:50:05.841+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:50:06.041+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:50:06.040+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:50:06.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 4.437 seconds
[2024-11-26T09:50:37.027+0000] {processor.py:161} INFO - Started process (PID=24858) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:50:37.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:50:37.030+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:50:37.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:50:40.347+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:50:42.383+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:50:42.412+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:50:42.411+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:50:42.595+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:50:42.595+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:50:42.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 5.597 seconds
[2024-11-26T09:51:13.274+0000] {processor.py:161} INFO - Started process (PID=25244) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:51:13.275+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:51:13.276+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:51:13.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:51:14.718+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:51:15.792+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:51:15.806+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:51:15.805+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:51:15.951+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:51:15.951+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:51:15.963+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 2.695 seconds
[2024-11-26T09:51:46.419+0000] {processor.py:161} INFO - Started process (PID=25523) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:51:46.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:51:46.422+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:51:46.422+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:51:48.270+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:51:49.528+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:51:49.546+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:51:49.546+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:51:49.567+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:51:49.567+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:51:49.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.172 seconds
[2024-11-26T09:52:20.204+0000] {processor.py:161} INFO - Started process (PID=25802) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:52:20.205+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:52:20.206+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:52:20.206+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:52:21.887+0000] {logging_mixin.py:188} WARNING - There was a problem when trying to write in your cache folder (/home/airflow/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[2024-11-26T09:52:23.115+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:52:23.128+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:52:23.127+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T09:52:23.144+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:52:23.144+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T09:52:23.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.100 seconds
[2024-11-26T09:52:53.696+0000] {processor.py:161} INFO - Started process (PID=26081) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:52:53.696+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:52:53.698+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:52:53.698+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:52:54.195+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:52:54.193+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:52:54.197+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:52:54.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.519 seconds
[2024-11-26T09:53:24.446+0000] {processor.py:161} INFO - Started process (PID=26349) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:53:24.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:53:24.448+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:53:24.448+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:53:24.981+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:53:24.979+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:53:24.982+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:53:27.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.095 seconds
[2024-11-26T09:53:58.090+0000] {processor.py:161} INFO - Started process (PID=26617) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:53:58.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:53:58.095+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:53:58.095+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:53:58.716+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:53:58.714+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:53:58.718+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:53:58.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.650 seconds
[2024-11-26T09:54:28.994+0000] {processor.py:161} INFO - Started process (PID=26885) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:54:28.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:54:28.996+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:54:28.996+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:54:29.604+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:54:29.602+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:54:29.606+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:54:29.619+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.631 seconds
[2024-11-26T09:55:00.008+0000] {processor.py:161} INFO - Started process (PID=27153) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:55:00.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:55:00.010+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:55:00.010+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:55:00.558+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:55:00.556+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:55:00.560+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:55:00.571+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.568 seconds
[2024-11-26T09:55:31.003+0000] {processor.py:161} INFO - Started process (PID=27421) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:55:31.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:55:31.005+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:55:31.004+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:55:31.500+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:55:31.498+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:55:31.502+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:55:31.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.514 seconds
[2024-11-26T09:56:01.806+0000] {processor.py:161} INFO - Started process (PID=27689) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:56:01.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:56:01.807+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:56:01.807+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:56:02.336+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:56:02.334+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:56:02.338+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:56:02.348+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.547 seconds
[2024-11-26T09:56:32.518+0000] {processor.py:161} INFO - Started process (PID=27957) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:56:32.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:56:32.520+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:56:32.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:56:33.096+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:56:33.094+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:56:33.099+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:56:33.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.598 seconds
[2024-11-26T09:57:03.432+0000] {processor.py:161} INFO - Started process (PID=28225) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:57:03.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:57:03.433+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:57:03.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:57:04.007+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:57:04.004+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:57:04.010+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:57:04.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.594 seconds
[2024-11-26T09:57:34.128+0000] {processor.py:161} INFO - Started process (PID=28494) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:57:34.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:57:34.130+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:57:34.129+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:57:34.662+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:57:34.659+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:57:34.663+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:57:34.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.553 seconds
[2024-11-26T09:58:04.739+0000] {processor.py:161} INFO - Started process (PID=28762) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:58:04.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:58:04.741+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:58:04.740+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:58:05.261+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:58:05.258+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:58:05.263+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:58:05.274+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.540 seconds
[2024-11-26T09:58:35.633+0000] {processor.py:161} INFO - Started process (PID=29030) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:58:35.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:58:35.634+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:58:35.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:58:36.227+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:58:36.225+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:58:36.229+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:58:36.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.613 seconds
[2024-11-26T09:59:06.597+0000] {processor.py:161} INFO - Started process (PID=29298) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:59:06.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:59:06.599+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:59:06.598+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:59:07.202+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:59:07.199+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:59:07.204+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:59:07.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.626 seconds
[2024-11-26T09:59:37.331+0000] {processor.py:161} INFO - Started process (PID=29566) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:59:37.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T09:59:37.333+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:59:37.333+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:59:37.902+0000] {logging_mixin.py:188} INFO - [2024-11-26T09:59:37.899+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T09:59:37.904+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T09:59:37.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.592 seconds
[2024-11-26T10:00:08.451+0000] {processor.py:161} INFO - Started process (PID=29834) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:00:08.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:00:08.453+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:00:08.453+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:00:09.090+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:00:09.088+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:00:09.093+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:00:09.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.660 seconds
[2024-11-26T10:00:39.191+0000] {processor.py:161} INFO - Started process (PID=30103) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:00:39.192+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:00:39.193+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:00:39.193+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:00:39.820+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:00:39.817+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:00:39.822+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:00:39.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.647 seconds
[2024-11-26T10:01:10.132+0000] {processor.py:161} INFO - Started process (PID=30371) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:01:10.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:01:10.134+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:01:10.133+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:01:10.741+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:01:10.738+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:01:10.743+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:01:10.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.628 seconds
[2024-11-26T10:01:41.629+0000] {processor.py:161} INFO - Started process (PID=30639) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:01:41.630+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:01:41.631+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:01:41.631+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:01:42.194+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:01:42.192+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:01:42.196+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:01:42.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.585 seconds
[2024-11-26T10:02:12.814+0000] {processor.py:161} INFO - Started process (PID=30907) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:02:12.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:02:12.816+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:02:12.816+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:02:13.380+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:02:13.378+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:02:13.382+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:02:13.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.587 seconds
[2024-11-26T10:02:43.884+0000] {processor.py:161} INFO - Started process (PID=31175) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:02:43.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:02:43.885+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:02:43.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:02:44.438+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:02:44.435+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:02:44.441+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:02:44.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.576 seconds
[2024-11-26T10:03:14.553+0000] {processor.py:161} INFO - Started process (PID=31443) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:03:14.554+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:03:14.555+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:03:14.554+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:03:15.079+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:03:15.076+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:03:15.080+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:03:15.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.544 seconds
[2024-11-26T10:03:45.666+0000] {processor.py:161} INFO - Started process (PID=31711) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:03:45.667+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:03:45.668+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:03:45.668+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:03:46.202+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:03:46.198+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:03:46.205+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:03:46.218+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.557 seconds
[2024-11-26T10:04:16.541+0000] {processor.py:161} INFO - Started process (PID=31980) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:04:16.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:04:16.542+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:04:16.542+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:04:17.088+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:04:17.086+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:04:17.090+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:04:17.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.566 seconds
[2024-11-26T10:04:47.829+0000] {processor.py:161} INFO - Started process (PID=32248) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:04:47.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T10:04:47.830+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:04:47.830+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:04:48.354+0000] {logging_mixin.py:188} INFO - [2024-11-26T10:04:48.351+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T10:04:48.356+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T10:04:48.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.544 seconds
[2024-11-26T14:11:16.072+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:11:16.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:11:16.079+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:11:16.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:11:19.900+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:11:19.871+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:11:19.925+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:11:19.989+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 3.926 seconds
[2024-11-26T14:11:50.121+0000] {processor.py:161} INFO - Started process (PID=523) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:11:50.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:11:50.124+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:11:50.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:11:50.752+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:11:50.749+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:11:50.754+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:11:50.767+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.652 seconds
[2024-11-26T14:12:20.847+0000] {processor.py:161} INFO - Started process (PID=778) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:12:20.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:12:20.849+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:12:20.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:12:21.456+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:12:21.453+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:12:21.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:12:21.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.630 seconds
[2024-11-26T14:12:51.894+0000] {processor.py:161} INFO - Started process (PID=1046) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:12:51.895+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:12:51.897+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:12:51.897+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:12:52.486+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:12:52.483+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:12:52.487+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:12:52.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.610 seconds
[2024-11-26T14:13:22.607+0000] {processor.py:161} INFO - Started process (PID=1344) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:13:22.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:13:22.610+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:13:22.609+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:13:23.259+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:13:23.256+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:13:23.262+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:13:23.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.671 seconds
[2024-11-26T14:13:53.599+0000] {processor.py:161} INFO - Started process (PID=1632) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:13:53.599+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:13:53.601+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:13:53.600+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:13:54.199+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:13:54.197+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:13:54.201+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:13:54.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.620 seconds
[2024-11-26T14:14:24.307+0000] {processor.py:161} INFO - Started process (PID=1930) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:14:24.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:14:24.310+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:14:24.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:14:24.918+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:14:24.915+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:14:24.919+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:14:24.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.630 seconds
[2024-11-26T14:14:55.077+0000] {processor.py:161} INFO - Started process (PID=2233) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:14:55.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:14:55.080+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:14:55.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:14:55.709+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:14:55.706+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:14:55.711+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:14:55.724+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.652 seconds
[2024-11-26T14:15:26.110+0000] {processor.py:161} INFO - Started process (PID=2524) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:15:26.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:15:26.113+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:15:26.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:15:26.757+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:15:26.754+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:15:26.759+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:15:26.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.668 seconds
[2024-11-26T14:15:57.460+0000] {processor.py:161} INFO - Started process (PID=2794) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:15:57.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:15:57.470+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:15:57.470+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:15:58.493+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:15:58.490+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:15:58.495+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:15:58.511+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 1.058 seconds
[2024-11-26T14:16:28.744+0000] {processor.py:161} INFO - Started process (PID=3062) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:16:28.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:16:28.747+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:16:28.747+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:16:29.654+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:16:29.649+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:16:29.657+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:16:29.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.937 seconds
[2024-11-26T14:17:00.055+0000] {processor.py:161} INFO - Started process (PID=3330) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:17:00.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:17:00.057+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:17:00.057+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:17:00.675+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:17:00.672+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:17:00.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:17:00.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.639 seconds
[2024-11-26T14:17:31.177+0000] {processor.py:161} INFO - Started process (PID=3598) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:17:31.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:17:31.180+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:17:31.179+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:17:31.834+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:17:31.831+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:17:31.836+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:17:31.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.678 seconds
[2024-11-26T14:18:02.312+0000] {processor.py:161} INFO - Started process (PID=3866) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:18:02.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:18:02.322+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:18:02.321+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:18:02.981+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:18:02.977+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:18:02.982+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:18:02.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.692 seconds
[2024-11-26T14:18:33.162+0000] {processor.py:161} INFO - Started process (PID=4134) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:18:33.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:18:33.164+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:18:33.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:18:33.736+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:18:33.734+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:18:33.739+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:18:33.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.594 seconds
[2024-11-26T14:19:04.023+0000] {processor.py:161} INFO - Started process (PID=4402) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:19:04.024+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:19:04.025+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:19:04.025+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:19:04.620+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:19:04.618+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:19:04.621+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:19:04.633+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.614 seconds
[2024-11-26T14:19:34.815+0000] {processor.py:161} INFO - Started process (PID=4670) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:19:34.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:19:34.818+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:19:34.817+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:19:35.386+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:19:35.383+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:19:35.388+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:19:35.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.589 seconds
[2024-11-26T14:20:05.556+0000] {processor.py:161} INFO - Started process (PID=4956) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:20:05.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:20:05.558+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:20:05.558+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:20:06.131+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:20:06.128+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:20:06.133+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:20:06.144+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.594 seconds
[2024-11-26T14:20:36.250+0000] {processor.py:161} INFO - Started process (PID=5206) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:20:36.251+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:20:36.253+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:20:36.252+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:20:36.977+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:20:36.973+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:20:36.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:20:36.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.748 seconds
[2024-11-26T14:21:07.105+0000] {processor.py:161} INFO - Started process (PID=5492) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:21:07.106+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:21:07.108+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:21:07.108+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:21:07.688+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:21:07.685+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:21:07.690+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:21:07.701+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.600 seconds
[2024-11-26T14:21:38.017+0000] {processor.py:161} INFO - Started process (PID=5785) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:21:38.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:21:38.020+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:21:38.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:21:38.617+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:21:38.614+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:21:38.619+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:21:38.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.618 seconds
[2024-11-26T14:22:09.035+0000] {processor.py:161} INFO - Started process (PID=6065) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:22:09.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:22:09.037+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:22:09.037+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:22:09.620+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:22:09.618+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:22:09.623+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:22:09.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.603 seconds
[2024-11-26T14:22:40.317+0000] {processor.py:161} INFO - Started process (PID=6333) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:22:40.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:22:40.320+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:22:40.320+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:22:40.991+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:22:40.988+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:22:40.993+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:22:41.008+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.696 seconds
[2024-11-26T14:23:11.192+0000] {processor.py:161} INFO - Started process (PID=6602) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:23:11.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:23:11.194+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:23:11.194+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:23:11.770+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:23:11.768+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:23:11.772+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:23:11.783+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.596 seconds
[2024-11-26T14:23:41.842+0000] {processor.py:161} INFO - Started process (PID=6870) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:23:41.842+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:23:41.844+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:23:41.844+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:23:42.601+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:23:42.598+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:23:42.604+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:23:42.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.781 seconds
[2024-11-26T14:24:12.769+0000] {processor.py:161} INFO - Started process (PID=7138) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:24:12.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:24:12.771+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:24:12.771+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:24:13.357+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:24:13.355+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:24:13.359+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:24:13.371+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.607 seconds
[2024-11-26T14:24:43.780+0000] {processor.py:161} INFO - Started process (PID=7412) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:24:43.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:24:43.783+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:24:43.782+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:24:44.526+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:24:44.523+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:24:44.528+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:24:44.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.767 seconds
[2024-11-26T14:25:14.618+0000] {processor.py:161} INFO - Started process (PID=7674) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:25:14.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:25:14.621+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:25:14.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:25:15.327+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:25:15.324+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:25:15.329+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:25:15.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.728 seconds
[2024-11-26T14:25:45.776+0000] {processor.py:161} INFO - Started process (PID=7942) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:25:45.776+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:25:45.778+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:25:45.778+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:25:46.406+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:25:46.403+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:25:46.408+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:25:46.420+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.650 seconds
[2024-11-26T14:26:16.587+0000] {processor.py:161} INFO - Started process (PID=8214) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:26:16.588+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:26:16.590+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:26:16.590+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:26:17.199+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:26:17.196+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:26:17.200+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:26:17.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.631 seconds
[2024-11-26T14:26:47.470+0000] {processor.py:161} INFO - Started process (PID=8484) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:26:47.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:26:47.472+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:26:47.472+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:26:48.090+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:26:48.087+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:26:48.092+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:26:48.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.639 seconds
[2024-11-26T14:27:18.649+0000] {processor.py:161} INFO - Started process (PID=8752) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:27:18.650+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:27:18.652+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:27:18.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:27:19.237+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:27:19.234+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:27:19.238+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:27:19.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.606 seconds
[2024-11-26T14:27:49.463+0000] {processor.py:161} INFO - Started process (PID=9020) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:27:49.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:27:49.466+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:27:49.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:27:50.082+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:27:50.078+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:27:50.086+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:27:50.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.641 seconds
[2024-11-26T14:28:20.517+0000] {processor.py:161} INFO - Started process (PID=9289) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:28:20.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:28:20.520+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:28:20.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:28:21.165+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:28:21.162+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:28:21.168+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:28:21.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.670 seconds
[2024-11-26T14:28:51.685+0000] {processor.py:161} INFO - Started process (PID=9558) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:28:51.686+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:28:51.688+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:28:51.687+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:28:52.163+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:28:52.160+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:28:52.164+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:28:52.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.501 seconds
[2024-11-26T14:29:22.362+0000] {processor.py:161} INFO - Started process (PID=9832) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:29:22.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:29:22.364+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:29:22.364+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:29:22.946+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:29:22.944+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:29:22.948+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:29:22.963+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.606 seconds
[2024-11-26T14:29:53.013+0000] {processor.py:161} INFO - Started process (PID=10098) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:29:53.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:29:53.015+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:29:53.015+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:29:53.527+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:29:53.524+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:29:53.529+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:29:53.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.536 seconds
[2024-11-26T14:30:24.170+0000] {processor.py:161} INFO - Started process (PID=10362) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:30:24.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:30:24.172+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:30:24.172+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:30:24.621+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:30:24.618+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:30:24.623+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:30:24.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.474 seconds
[2024-11-26T14:30:54.757+0000] {processor.py:161} INFO - Started process (PID=10636) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:30:54.758+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:30:54.760+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:30:54.760+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:30:55.321+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:30:55.317+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:30:55.323+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:30:55.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.591 seconds
[2024-11-26T14:31:25.485+0000] {processor.py:161} INFO - Started process (PID=10898) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:31:25.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:31:25.488+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:31:25.487+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:31:26.169+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:31:26.167+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:31:26.171+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:31:26.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.707 seconds
[2024-11-26T14:31:56.635+0000] {processor.py:161} INFO - Started process (PID=11166) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:31:56.636+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:31:56.639+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:31:56.639+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:31:57.007+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:31:57.002+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:31:57.011+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:31:57.146+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.678 seconds
[2024-11-26T14:32:27.534+0000] {processor.py:161} INFO - Started process (PID=11434) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:32:27.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:32:27.536+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:32:27.536+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:32:28.034+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:32:28.030+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:32:28.037+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:32:28.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.524 seconds
[2024-11-26T14:32:58.213+0000] {processor.py:161} INFO - Started process (PID=11706) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:32:58.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:32:58.215+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:32:58.215+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:32:58.651+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:32:58.648+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:32:58.652+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:32:58.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.457 seconds
[2024-11-26T14:33:28.937+0000] {processor.py:161} INFO - Started process (PID=11970) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:33:28.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:33:28.940+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:33:28.940+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:33:29.631+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:33:29.627+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ImportError: cannot import name 'process_and_insert_data' from 'tasks.spark_milvus_task' (/opt/airflow/dags/tasks/spark_milvus_task.py)
[2024-11-26T14:33:29.635+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:33:29.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.720 seconds
[2024-11-26T14:33:59.786+0000] {processor.py:161} INFO - Started process (PID=12238) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:33:59.787+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:33:59.789+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:33:59.788+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:33:59.878+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:33:59.875+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:33:59.879+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:33:59.894+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.113 seconds
[2024-11-26T14:34:30.044+0000] {processor.py:161} INFO - Started process (PID=12505) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:34:30.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:34:30.047+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:34:30.047+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:34:30.130+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:34:30.127+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:34:30.132+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:34:30.146+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T14:35:00.260+0000] {processor.py:161} INFO - Started process (PID=12772) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:35:00.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:35:00.264+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:35:00.264+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:35:00.348+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:35:00.346+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:35:00.349+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:35:00.362+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T14:35:30.494+0000] {processor.py:161} INFO - Started process (PID=13039) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:35:30.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:35:30.497+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:35:30.497+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:35:30.578+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:35:30.576+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:35:30.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:35:30.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.104 seconds
[2024-11-26T14:36:00.647+0000] {processor.py:161} INFO - Started process (PID=13306) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:36:00.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:36:00.651+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:36:00.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:36:00.734+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:36:00.731+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:36:00.735+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:36:00.749+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T14:36:30.945+0000] {processor.py:161} INFO - Started process (PID=13573) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:36:30.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:36:30.948+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:36:30.947+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:36:31.032+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:36:31.029+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:36:31.032+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:36:31.047+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T14:37:01.166+0000] {processor.py:161} INFO - Started process (PID=13840) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:37:01.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:37:01.169+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:37:01.168+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:37:01.255+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:37:01.252+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:37:01.256+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:37:01.269+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T14:37:31.499+0000] {processor.py:161} INFO - Started process (PID=14107) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:37:31.500+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:37:31.502+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:37:31.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:37:31.588+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:37:31.585+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:37:31.589+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:37:31.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T14:38:01.710+0000] {processor.py:161} INFO - Started process (PID=14374) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:38:01.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:38:01.712+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:38:01.712+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:38:01.797+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:38:01.794+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:38:01.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:38:01.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T14:38:31.846+0000] {processor.py:161} INFO - Started process (PID=14641) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:38:31.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:38:31.849+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:38:31.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:38:31.936+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:38:31.933+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:38:31.937+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:38:31.953+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.111 seconds
[2024-11-26T14:39:02.104+0000] {processor.py:161} INFO - Started process (PID=14908) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:39:02.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:39:02.107+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:39:02.107+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:39:02.191+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:39:02.189+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:39:02.192+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:39:02.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T14:39:32.369+0000] {processor.py:161} INFO - Started process (PID=15175) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:39:32.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:39:32.371+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:39:32.371+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:39:32.455+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:39:32.452+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:39:32.456+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:39:32.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.105 seconds
[2024-11-26T14:40:02.668+0000] {processor.py:161} INFO - Started process (PID=15442) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:40:02.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:40:02.671+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:40:02.671+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:40:02.757+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:40:02.754+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:40:02.758+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:40:02.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.109 seconds
[2024-11-26T14:40:32.944+0000] {processor.py:161} INFO - Started process (PID=15709) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:40:32.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:40:32.947+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:40:32.947+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:40:33.028+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:40:33.026+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:40:33.029+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:40:33.043+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.104 seconds
[2024-11-26T14:41:03.206+0000] {processor.py:161} INFO - Started process (PID=15976) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:41:03.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:41:03.209+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:41:03.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:41:03.291+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:41:03.289+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:41:03.292+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:41:03.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.106 seconds
[2024-11-26T14:41:33.591+0000] {processor.py:161} INFO - Started process (PID=16243) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:41:33.592+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:41:33.594+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:41:33.594+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:41:33.692+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:41:33.688+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:41:33.693+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:41:33.716+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.130 seconds
[2024-11-26T14:42:03.908+0000] {processor.py:161} INFO - Started process (PID=16510) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:42:03.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:42:03.912+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:42:03.911+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:42:04.000+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:42:03.997+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:42:04.001+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:42:04.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.113 seconds
[2024-11-26T14:42:34.144+0000] {processor.py:161} INFO - Started process (PID=16777) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:42:34.145+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:42:34.147+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:42:34.147+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:42:34.234+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:42:34.231+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:42:34.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:42:34.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.110 seconds
[2024-11-26T14:43:04.322+0000] {processor.py:161} INFO - Started process (PID=17044) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:43:04.323+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:43:04.325+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:43:04.325+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:43:04.414+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:43:04.411+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:43:04.415+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:43:04.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.113 seconds
[2024-11-26T14:43:34.567+0000] {processor.py:161} INFO - Started process (PID=17311) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:43:34.568+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:43:34.570+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:43:34.570+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:43:34.662+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:43:34.659+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:43:34.663+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:43:34.680+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.119 seconds
[2024-11-26T14:44:04.717+0000] {processor.py:161} INFO - Started process (PID=17578) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:44:04.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:44:04.720+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:44:04.720+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:44:04.805+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:44:04.802+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:44:04.806+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:44:04.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.109 seconds
[2024-11-26T14:44:34.922+0000] {processor.py:161} INFO - Started process (PID=17845) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:44:34.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:44:34.925+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:44:34.925+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:44:35.009+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:44:35.007+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:44:35.010+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:44:35.025+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T14:45:05.088+0000] {processor.py:161} INFO - Started process (PID=18112) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:45:05.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:45:05.091+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:45:05.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:45:05.187+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:45:05.184+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:45:05.188+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:45:05.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.119 seconds
[2024-11-26T14:45:35.348+0000] {processor.py:161} INFO - Started process (PID=18379) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:45:35.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:45:35.351+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:45:35.351+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:45:35.434+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:45:35.432+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:45:35.435+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:45:35.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.106 seconds
[2024-11-26T14:46:05.868+0000] {processor.py:161} INFO - Started process (PID=18647) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:46:05.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:46:05.871+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:46:05.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:46:05.959+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:46:05.957+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:46:05.960+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:46:05.976+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.112 seconds
[2024-11-26T14:46:36.088+0000] {processor.py:161} INFO - Started process (PID=18914) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:46:36.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:46:36.091+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:46:36.090+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:46:36.176+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:46:36.174+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:46:36.178+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:46:36.192+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.109 seconds
[2024-11-26T14:47:06.596+0000] {processor.py:161} INFO - Started process (PID=19181) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:47:06.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:47:06.599+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:47:06.599+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:47:06.699+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:47:06.696+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:47:06.700+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:47:06.716+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.124 seconds
[2024-11-26T14:47:36.837+0000] {processor.py:161} INFO - Started process (PID=19448) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:47:36.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:47:36.840+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:47:36.840+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:47:36.923+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:47:36.921+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:47:36.924+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:47:36.937+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.106 seconds
[2024-11-26T14:48:07.105+0000] {processor.py:161} INFO - Started process (PID=19715) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:48:07.106+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:48:07.108+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:48:07.108+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:48:07.198+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:48:07.196+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:48:07.199+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:48:07.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.113 seconds
[2024-11-26T14:48:37.517+0000] {processor.py:161} INFO - Started process (PID=19982) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:48:37.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:48:37.519+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:48:37.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:48:37.605+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:48:37.603+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:48:37.606+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:48:37.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.109 seconds
[2024-11-26T14:49:07.667+0000] {processor.py:161} INFO - Started process (PID=20244) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:49:07.668+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:49:07.670+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:49:07.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:49:07.785+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:49:07.781+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:49:07.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:49:07.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.143 seconds
[2024-11-26T14:49:37.907+0000] {processor.py:161} INFO - Started process (PID=20511) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:49:37.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:49:37.910+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:49:37.909+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:49:37.998+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:49:37.995+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:49:37.999+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:49:38.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.110 seconds
[2024-11-26T14:50:08.206+0000] {processor.py:161} INFO - Started process (PID=20779) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:50:08.206+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:50:08.209+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:50:08.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:50:08.293+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:50:08.291+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:50:08.294+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:50:08.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.106 seconds
[2024-11-26T14:50:38.652+0000] {processor.py:161} INFO - Started process (PID=21046) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:50:38.652+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:50:38.655+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:50:38.655+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:50:38.746+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:50:38.742+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:50:38.747+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:50:38.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.115 seconds
[2024-11-26T14:51:09.038+0000] {processor.py:161} INFO - Started process (PID=21313) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:51:09.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:51:09.041+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:51:09.040+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:51:09.160+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:51:09.157+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:51:09.161+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:51:09.176+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.145 seconds
[2024-11-26T14:51:39.679+0000] {processor.py:161} INFO - Started process (PID=21580) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:51:39.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:51:39.682+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:51:39.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:51:39.775+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:51:39.772+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:51:39.776+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:51:39.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.116 seconds
[2024-11-26T14:52:09.885+0000] {processor.py:161} INFO - Started process (PID=21848) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:52:09.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:52:09.888+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:52:09.888+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:52:09.984+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:52:09.982+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:52:09.986+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:52:10.000+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.120 seconds
[2024-11-26T14:52:40.291+0000] {processor.py:161} INFO - Started process (PID=22121) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:52:40.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:52:40.296+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:52:40.295+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:52:40.505+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:52:40.499+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:52:40.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:52:40.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.244 seconds
[2024-11-26T14:53:10.816+0000] {processor.py:161} INFO - Started process (PID=22388) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:53:10.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:53:10.819+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:53:10.818+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:53:10.907+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:53:10.904+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:53:10.908+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:53:10.930+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.120 seconds
[2024-11-26T14:53:41.137+0000] {processor.py:161} INFO - Started process (PID=22655) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:53:41.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:53:41.140+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:53:41.140+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:53:41.227+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:53:41.224+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:53:41.228+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:53:41.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.112 seconds
[2024-11-26T14:54:11.355+0000] {processor.py:161} INFO - Started process (PID=22922) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:54:11.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:54:11.358+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:54:11.357+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:54:11.449+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:54:11.447+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:54:11.450+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:54:11.467+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.117 seconds
[2024-11-26T14:54:41.538+0000] {processor.py:161} INFO - Started process (PID=23189) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:54:41.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:54:41.541+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:54:41.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:54:41.625+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:54:41.623+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:54:41.627+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:54:41.640+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T14:55:11.719+0000] {processor.py:161} INFO - Started process (PID=23456) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:55:11.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:55:11.721+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:55:11.721+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:55:11.810+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:55:11.807+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:55:11.811+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:55:11.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.111 seconds
[2024-11-26T14:55:41.926+0000] {processor.py:161} INFO - Started process (PID=23723) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:55:41.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:55:41.929+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:55:41.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:55:42.019+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:55:42.017+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:55:42.020+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:55:42.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.115 seconds
[2024-11-26T14:56:12.130+0000] {processor.py:161} INFO - Started process (PID=23990) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:56:12.131+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:56:12.133+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:56:12.133+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:56:12.227+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:56:12.224+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:56:12.228+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:56:12.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.118 seconds
[2024-11-26T14:56:42.688+0000] {processor.py:161} INFO - Started process (PID=24257) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:56:42.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:56:42.691+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:56:42.691+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:56:42.784+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:56:42.782+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:56:42.785+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:56:42.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.116 seconds
[2024-11-26T14:57:12.916+0000] {processor.py:161} INFO - Started process (PID=24524) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:57:12.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:57:12.919+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:57:12.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:57:13.003+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:57:13.000+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:57:13.004+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:57:13.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.110 seconds
[2024-11-26T14:57:43.111+0000] {processor.py:161} INFO - Started process (PID=24791) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:57:43.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:57:43.114+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:57:43.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:57:43.230+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:57:43.226+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:57:43.231+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:57:43.252+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.145 seconds
[2024-11-26T14:58:13.323+0000] {processor.py:161} INFO - Started process (PID=25058) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:58:13.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:58:13.327+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:58:13.327+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:58:13.426+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:58:13.423+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:58:13.427+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:58:13.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.129 seconds
[2024-11-26T14:58:43.571+0000] {processor.py:161} INFO - Started process (PID=25325) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:58:43.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:58:43.574+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:58:43.574+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:58:43.671+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:58:43.669+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:58:43.672+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:58:43.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.122 seconds
[2024-11-26T14:59:13.804+0000] {processor.py:161} INFO - Started process (PID=25592) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:59:13.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:59:13.807+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:59:13.806+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:59:13.898+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:59:13.895+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:59:13.899+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:59:13.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.115 seconds
[2024-11-26T14:59:44.275+0000] {processor.py:161} INFO - Started process (PID=25859) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:59:44.276+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T14:59:44.278+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:59:44.278+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:59:44.368+0000] {logging_mixin.py:188} INFO - [2024-11-26T14:59:44.366+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T14:59:44.369+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T14:59:44.382+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.112 seconds
[2024-11-26T15:00:14.754+0000] {processor.py:161} INFO - Started process (PID=26126) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:00:14.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:00:14.757+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:00:14.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:00:14.844+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:00:14.841+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:00:14.845+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:00:14.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.110 seconds
[2024-11-26T15:00:45.003+0000] {processor.py:161} INFO - Started process (PID=26393) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:00:45.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:00:45.006+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:00:45.005+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:00:45.096+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:00:45.093+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:00:45.097+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:00:45.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.114 seconds
[2024-11-26T15:01:15.201+0000] {processor.py:161} INFO - Started process (PID=26660) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:01:15.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:01:15.203+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:01:15.203+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:01:15.290+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:01:15.287+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:01:15.291+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:01:15.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.111 seconds
[2024-11-26T15:01:45.455+0000] {processor.py:161} INFO - Started process (PID=26927) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:01:45.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:01:45.457+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:01:45.457+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:01:45.541+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:01:45.539+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:01:45.542+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:01:45.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T15:02:15.863+0000] {processor.py:161} INFO - Started process (PID=27194) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:02:15.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:02:15.865+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:02:15.865+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:02:15.952+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:02:15.949+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:02:15.952+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:02:15.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T15:02:46.002+0000] {processor.py:161} INFO - Started process (PID=27461) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:02:46.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:02:46.005+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:02:46.005+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:02:46.092+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:02:46.090+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:02:46.093+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:02:46.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.113 seconds
[2024-11-26T15:03:16.281+0000] {processor.py:161} INFO - Started process (PID=27722) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:03:16.282+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:03:16.284+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:03:16.283+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:03:16.500+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:03:16.498+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:03:16.501+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:03:16.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.239 seconds
[2024-11-26T15:03:47.300+0000] {processor.py:161} INFO - Started process (PID=27989) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:03:47.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:03:47.302+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:03:47.302+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:03:47.390+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:03:47.387+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:03:47.391+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:03:47.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.111 seconds
[2024-11-26T15:04:17.739+0000] {processor.py:161} INFO - Started process (PID=28256) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:04:17.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:04:17.742+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:04:17.742+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:04:17.825+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:04:17.823+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:04:17.826+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:04:17.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.107 seconds
[2024-11-26T15:04:47.911+0000] {processor.py:161} INFO - Started process (PID=28524) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:04:47.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:04:47.913+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:04:47.913+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:04:47.997+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:04:47.995+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:04:47.998+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:04:48.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.104 seconds
[2024-11-26T15:05:18.090+0000] {processor.py:161} INFO - Started process (PID=28791) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:05:18.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:05:18.093+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:05:18.093+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:05:18.279+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:05:18.277+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:05:18.279+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:05:18.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.206 seconds
[2024-11-26T15:05:48.384+0000] {processor.py:161} INFO - Started process (PID=29058) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:05:48.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:05:48.386+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:05:48.386+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:05:48.594+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:05:48.591+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:05:48.595+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:05:48.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.230 seconds
[2024-11-26T15:06:19.633+0000] {processor.py:161} INFO - Started process (PID=29325) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:06:19.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:06:19.635+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:06:19.635+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:06:19.820+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:06:19.818+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:06:19.821+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:06:19.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.205 seconds
[2024-11-26T15:06:50.758+0000] {processor.py:161} INFO - Started process (PID=29593) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:06:50.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:06:50.761+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:06:50.761+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:06:50.846+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:06:50.844+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:06:50.847+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:06:50.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T15:07:21.167+0000] {processor.py:161} INFO - Started process (PID=29860) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:07:21.168+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:07:21.170+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:07:21.170+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:07:21.258+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:07:21.256+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:07:21.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:07:21.274+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.113 seconds
[2024-11-26T15:07:51.519+0000] {processor.py:161} INFO - Started process (PID=30127) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:07:51.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:07:51.523+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:07:51.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:07:51.718+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:07:51.716+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:07:51.719+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:07:51.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.218 seconds
[2024-11-26T15:08:22.644+0000] {processor.py:161} INFO - Started process (PID=30399) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:08:22.645+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:08:22.646+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:08:22.646+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:08:22.843+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:08:22.840+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:08:22.844+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:08:22.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.216 seconds
[2024-11-26T15:08:52.925+0000] {processor.py:161} INFO - Started process (PID=30667) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:08:52.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:08:52.928+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:08:52.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:08:53.132+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:08:53.130+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:08:53.133+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:08:53.146+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.229 seconds
[2024-11-26T15:09:23.244+0000] {processor.py:161} INFO - Started process (PID=30934) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:09:23.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:09:23.247+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:09:23.247+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:09:23.443+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:09:23.441+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:09:23.444+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:09:23.455+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.217 seconds
[2024-11-26T15:09:54.010+0000] {processor.py:161} INFO - Started process (PID=31201) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:09:54.011+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:09:54.012+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:09:54.012+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:09:54.212+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:09:54.209+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:09:54.213+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:09:54.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.220 seconds
[2024-11-26T15:10:24.449+0000] {processor.py:161} INFO - Started process (PID=31478) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:10:24.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:10:24.451+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:10:24.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:10:24.633+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:10:24.630+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:10:24.633+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:10:24.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.202 seconds
[2024-11-26T15:10:54.694+0000] {processor.py:161} INFO - Started process (PID=31745) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:10:54.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:10:54.696+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:10:54.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:10:54.781+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:10:54.779+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:10:54.782+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:10:54.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T15:11:24.935+0000] {processor.py:161} INFO - Started process (PID=32002) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:11:24.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:11:24.938+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:11:24.937+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:11:25.030+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:11:25.027+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:11:25.031+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:11:25.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.116 seconds
[2024-11-26T15:11:55.097+0000] {processor.py:161} INFO - Started process (PID=32269) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:11:55.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:11:55.100+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:11:55.100+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:11:55.188+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:11:55.185+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:11:55.189+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:11:55.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.115 seconds
[2024-11-26T15:12:25.470+0000] {processor.py:161} INFO - Started process (PID=32536) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:12:25.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:12:25.472+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:12:25.472+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:12:25.561+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:12:25.558+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:12:25.562+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:12:25.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.112 seconds
[2024-11-26T15:12:55.680+0000] {processor.py:161} INFO - Started process (PID=32803) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:12:55.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:12:55.683+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:12:55.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:12:55.773+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:12:55.770+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:12:55.774+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:12:55.788+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.114 seconds
[2024-11-26T15:13:25.880+0000] {processor.py:161} INFO - Started process (PID=33070) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:13:25.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:13:25.883+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:13:25.883+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:13:25.974+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:13:25.971+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:13:25.975+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:13:25.989+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.114 seconds
[2024-11-26T15:13:56.082+0000] {processor.py:161} INFO - Started process (PID=33337) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:13:56.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:13:56.085+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:13:56.085+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:13:56.177+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:13:56.174+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:13:56.178+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:13:56.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.118 seconds
[2024-11-26T15:14:26.311+0000] {processor.py:161} INFO - Started process (PID=33604) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:14:26.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:14:26.313+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:14:26.313+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:14:26.409+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:14:26.406+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:14:26.410+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:14:26.423+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.117 seconds
[2024-11-26T15:14:56.618+0000] {processor.py:161} INFO - Started process (PID=33871) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:14:56.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:14:56.621+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:14:56.621+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:14:56.716+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:14:56.713+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:14:56.718+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:14:56.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.120 seconds
[2024-11-26T15:15:26.914+0000] {processor.py:161} INFO - Started process (PID=34138) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:15:26.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:15:26.917+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:15:26.916+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:15:27.011+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:15:27.008+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:15:27.012+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:15:27.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.118 seconds
[2024-11-26T15:15:57.927+0000] {processor.py:161} INFO - Started process (PID=34405) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:15:57.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:15:57.929+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:15:57.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:15:58.015+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:15:58.013+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:15:58.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:15:58.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.108 seconds
[2024-11-26T15:16:28.413+0000] {processor.py:161} INFO - Started process (PID=34672) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:16:28.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:16:28.416+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:16:28.416+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:16:28.508+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:16:28.505+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:16:28.509+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:16:28.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.117 seconds
[2024-11-26T15:16:58.809+0000] {processor.py:161} INFO - Started process (PID=34939) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:16:58.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:16:58.811+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:16:58.811+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:16:58.903+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:16:58.900+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:16:58.904+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:16:58.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.114 seconds
[2024-11-26T15:17:29.044+0000] {processor.py:161} INFO - Started process (PID=35207) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:17:29.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:17:29.047+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:17:29.046+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:17:29.145+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:17:29.141+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:17:29.146+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:17:29.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.128 seconds
[2024-11-26T15:17:59.487+0000] {processor.py:161} INFO - Started process (PID=35474) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:17:59.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:17:59.489+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:17:59.489+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:17:59.625+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:17:59.621+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:17:59.627+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:17:59.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.166 seconds
[2024-11-26T15:18:29.779+0000] {processor.py:161} INFO - Started process (PID=35741) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:18:29.780+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:18:29.782+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:18:29.781+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:18:29.887+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:18:29.883+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:18:29.888+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:18:29.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.133 seconds
[2024-11-26T15:19:00.575+0000] {processor.py:161} INFO - Started process (PID=36008) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:19:00.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:19:00.578+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:19:00.577+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:19:00.672+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:19:00.668+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:19:00.673+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:19:00.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.116 seconds
[2024-11-26T15:19:30.799+0000] {processor.py:161} INFO - Started process (PID=36275) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:19:30.800+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:19:30.801+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:19:30.801+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:19:30.893+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:19:30.890+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:19:30.894+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:19:30.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.114 seconds
[2024-11-26T15:20:01.166+0000] {processor.py:161} INFO - Started process (PID=36542) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:20:01.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:20:01.168+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:20:01.168+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:20:01.265+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:20:01.262+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:20:01.266+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:20:01.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.124 seconds
[2024-11-26T15:20:31.709+0000] {processor.py:161} INFO - Started process (PID=36815) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:20:31.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:20:31.712+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:20:31.711+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:20:31.802+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:20:31.799+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:20:31.803+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:20:31.818+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.115 seconds
[2024-11-26T15:21:01.884+0000] {processor.py:161} INFO - Started process (PID=37082) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:21:01.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:21:01.887+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:21:01.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:21:01.979+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:21:01.975+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:21:01.980+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:21:01.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.117 seconds
[2024-11-26T15:21:32.219+0000] {processor.py:161} INFO - Started process (PID=37349) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:21:32.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:21:32.222+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:21:32.222+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:21:32.325+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:21:32.321+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:21:32.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:21:32.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.129 seconds
[2024-11-26T15:22:02.381+0000] {processor.py:161} INFO - Started process (PID=37616) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:22:02.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:22:02.384+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:22:02.384+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:22:02.486+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:22:02.483+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:22:02.487+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:22:02.501+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.127 seconds
[2024-11-26T15:22:32.618+0000] {processor.py:161} INFO - Started process (PID=37883) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:22:32.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:22:32.620+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:22:32.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:22:32.719+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:22:32.716+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:22:32.720+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:22:32.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.122 seconds
[2024-11-26T15:23:02.963+0000] {processor.py:161} INFO - Started process (PID=38150) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:02.964+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:23:02.965+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:23:02.965+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:03.059+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:23:03.057+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:23:03.060+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:03.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.117 seconds
[2024-11-26T15:23:14.564+0000] {processor.py:161} INFO - Started process (PID=38310) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:14.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:23:14.566+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:23:14.566+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:14.690+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:23:14.684+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:23:14.694+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:14.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.169 seconds
[2024-11-26T15:23:45.015+0000] {processor.py:161} INFO - Started process (PID=38577) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:45.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:23:45.018+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:23:45.018+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:45.121+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:23:45.118+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:23:45.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:23:45.140+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.130 seconds
[2024-11-26T15:24:15.366+0000] {processor.py:161} INFO - Started process (PID=38844) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:15.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:24:15.368+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:24:15.368+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:15.464+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:24:15.461+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:24:15.465+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:15.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.121 seconds
[2024-11-26T15:24:46.011+0000] {processor.py:161} INFO - Started process (PID=39111) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:46.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:24:46.015+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:24:46.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:46.143+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:24:46.139+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/amazon_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/amazon_pipeline_dag.py", line 5, in <module>
    from tasks.spark_milvus_task import process_and_insert_data
ModuleNotFoundError: No module named 'tasks.spark_milvus_task'
[2024-11-26T15:24:46.145+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:46.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.166 seconds
[2024-11-26T15:24:53.652+0000] {processor.py:161} INFO - Started process (PID=39182) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:53.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:24:53.654+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:24:53.654+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:53.788+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:24:53.899+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:24:53.899+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T15:24:53.921+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:24:53.921+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T15:24:53.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.302 seconds
[2024-11-26T15:25:24.152+0000] {processor.py:161} INFO - Started process (PID=39449) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:25:24.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:25:24.155+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:25:24.154+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:25:24.261+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:25:24.280+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:25:24.279+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T15:25:24.302+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:25:24.302+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T15:25:24.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.177 seconds
[2024-11-26T15:25:54.873+0000] {processor.py:161} INFO - Started process (PID=39716) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:25:54.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:25:54.880+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:25:54.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:25:55.023+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:25:55.048+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:25:55.047+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T15:25:55.080+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:25:55.079+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T15:25:55.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.244 seconds
[2024-11-26T15:26:25.644+0000] {processor.py:161} INFO - Started process (PID=39983) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:26:25.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:26:25.646+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:26:25.646+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:26:25.741+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:26:25.755+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:26:25.754+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T15:26:25.772+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:26:25.772+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T15:26:25.786+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.149 seconds
[2024-11-26T15:26:56.027+0000] {processor.py:161} INFO - Started process (PID=40250) to work on /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:26:56.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/amazon_pipeline_dag.py for tasks to queue
[2024-11-26T15:26:56.030+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:26:56.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:26:56.132+0000] {processor.py:840} INFO - DAG(s) 'amazon_pipeline' retrieved from /opt/airflow/dags/amazon_pipeline_dag.py
[2024-11-26T15:26:56.145+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:26:56.145+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-11-26T15:26:56.165+0000] {logging_mixin.py:188} INFO - [2024-11-26T15:26:56.164+0000] {dag.py:3954} INFO - Setting next_dagrun for amazon_pipeline to 2024-11-26 00:00:00+00:00, run_after=2024-11-27 00:00:00+00:00
[2024-11-26T15:26:56.180+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/amazon_pipeline_dag.py took 0.158 seconds
